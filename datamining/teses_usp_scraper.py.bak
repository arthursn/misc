# -*- coding: utf-8 -*-

import sys

import requests
from bs4 import BeautifulSoup  # Python library for pulling data out of HTML and XML files

def doi2bib(doi, timeout=5.):
    """
    Return a bibTeX string of metadata for a given DOI.
    https://gist.github.com/jrsmith3/5513926
    """
    url = "http://dx.doi.org/" + doi
    headers = {"accept": "application/x-bibtex"}
    r = requests.get(url, headers=headers, timeout=timeout)
    if r.status_code == 200:
        return r.text

def scrape_teses_usp_doi(url):
    """
    gets doi from thesis url
    """
    doi = None
    r = requests.get(url)
    if r.status_code == 200:  # success!
        soup = BeautifulSoup(r.content, "html.parser")
        titlefields = soup.find_all("div", {"class": "DocumentoTituloTexto"})
        for i in range(len(titlefields)):
            if titlefields[i].string.upper() == "DOI":
                doi = soup.find_all("div", {"class": "DocumentoTexto"})[i].string
                break
    return doi

def scrape_teses_usp(url, paginas=float("inf")):
    newentries = True
    pagina = 1
    thesis_data = []
    while newentries and pagina <= paginas:
        newentries = False
        r = requests.get(url + "&pagina={:d}".format(pagina))
        if r.status_code == 200:  # success!
            soup = BeautifulSoup(r.content, "html.parser")
            for row in soup.find_all("div", {"class": "dadosLinha"}):
                try:
                    name = row.find("div", {"class": "dadosDocNome"}).find("a")  # fetches a tag containing author name
                    title = row.find("div", {"class": "dadosDocTitulo"}).string  # document title
                    area = row.find("div", {"class": "dadosDocArea"}).find("a").string  # area
                    document = row.find("div", {"class": "dadosDocTipo"}).find("a").string  # document type (dissertation, thesis)
                    faculty = row.find("div", {"class": "dadosDocUnidade"}).find("a").string  # faculty
                    year = row.find("div", {"class": "dadosDocAno"}).find("a").string  # year

                    thesis_url = name.get("href")  # url
                    name = name.string  # author name
                    sys.stdout.write("Scrapping " + thesis_url + " | ")
                    sys.stdout.write("Getting doi... ")
                    thesis_doi = scrape_teses_usp_doi(thesis_url)
                    if thesis_doi:
                        sys.stdout.write("found: " + thesis_doi + "\n")
                    else:
                        sys.stdout.write("not found\n")
                    
                    thesis_data.append([name, title, area, document, faculty, year, thesis_url, thesis_doi])
                    newentries = True
                except:
                    pass
        
        pagina += 1

    return thesis_data

def data2csv(thesis_data):
    csv = []
    for entry in thesis_data:
        entry = ["\""+s+"\"" if s else "" for s in entry]
        csv.append(";".join(entry))
    return "name;title;area;document;faculty;year;url;doi\n" + "\n".join(csv)

# fnameout = "teses_ltf.csv"
# thesis_data = scrape_teses_usp("http://www.teses.usp.br/index.php?option=com_jumi&fileid=14&Itemid=161&id=E7A0B2A3748E&lang=pt-br")  # Teses orientadas pelo HÃ©lio

fnameout = "teses_pmt.csv"
thesis_data = scrape_teses_usp("http://www.teses.usp.br/index.php?option=com_jumi&fileid=9&Itemid=159&lang=en&id=3133&prog=3008")  # Metallurgical and Materials Engineering

fout = open(fnameout, "w")
fout.write(data2csv(thesis_data))
fout.close()

# fout = open("teses_ltf.bib", "w")

# for url, doi in thesis_data:
#     if doi:
#         bibtex = doi2bib(doi)
#         if bibtex:
#             print("Processing DOI {}".format(doi))
#             fout.write(bibtex)
#             fout.write('\n\n')
#             fout.flush()
#         else:
#             print("DOI {} not found".format(doi))

# fout.close()
